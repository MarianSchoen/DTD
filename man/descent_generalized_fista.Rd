% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/function_FISTA.R
\name{descent_generalized_fista}
\alias{descent_generalized_fista}
\title{Descent generalized FISTA \cr
(fast iterative shrinkage thresholding algorithm)}
\usage{
descent_generalized_fista(tweak_vec = NA, lambda = 0, maxit = 1000,
  learning.rate = NA, F.GRAD.FUN = DTD.grad.wrapper,
  ST.FUN = soft_thresholding, FACTOR.FUN = nesterov_faktor,
  EVAL.FUN = DTD.evCor.wrapper, line_search_speed = 2, cycles = 50,
  save_all_tweaks = F, verbose = T)
}
\arguments{
\item{tweak_vec}{numeric vector, with which the minimization algorithm starts}

\item{lambda}{float, regularization factor for ST.FUN function}

\item{maxit}{integer, maximal number of iterations for the iterative
minimization}

\item{learning.rate}{float, step size while learning}

\item{F.GRAD.FUN}{function with one parameter: vector with same length as
tweak_vec, and one return argument: vector with same length as tweak_vec
For a given vector, it returns the gradient of the loss-function}

\item{ST.FUN}{function, with one parameter: vector with same length as
tweak_vec, and one return argument: vector with same length as tweak_vec
implementation of the proximal operator for the chosen Loss-function. Here
named soft thresholding function.}

\item{FACTOR.FUN}{function, with a integer as input, and a float as return
value. This function returns the factor the nesterov correction
extrapolates.}

\item{EVAL.FUN}{fucntion, with one parameter: vector with same length as
tweak_vec, and a single float as return. This function evaluates the loss
function.}

\item{line_search_speed}{numeric, factor with which the learning rate changes,
if the optimium has not been found}

\item{cycles}{integer, in each iteration one gradient is calculated. To find the
best step size, with "cycles" different step sizes}

\item{save_all_tweaks}{boolean, should all tweak vectores during all iterations be stored}

\item{verbose}{boolean, should information be printed to console}
}
\value{
list, including [["Converge"]] and [["Tweak"]]
}
\description{
descent_generalized_fista takes as input a vector, a gradient function and an
evaluation function (and some additional parameters/functions). Then, it
iteratively minimizes the tweak vector via the FISTA algorithms. Basically,
the following equations are used:
}
\details{
The gradient and evaluation function both take only one argument, the soft thresholding function two.
If your gradient, evaluation or soft thresholding function require more arguments, please write a wrapper.
Exemplary wrapper functions can be found in the examples.
}
\examples{
library(DTD)
random.data <- generate.random.data(nTypes = 5,
                                    nSamples.perType = 10,
                                    nFeatures = 100,
                                    sample.type = "Cell",
                                    feature.type = "gene")

# normalize all samples to the same amount of counts:
random.data <- normalizeToCount(random.data)

# extract indicator list.
# This list contains the Type of the sample as value, and the sample name as name
indicator.list <- gsub("^Cell([0-9])*.", "", colnames(random.data))
names(indicator.list) <- colnames(random.data)

# extract reference matrix X
# First, decide which cells should be deconvoluted.
# Notice, in the mixtures there can be more cells than in the reference matrix.
include.in.X <- c("Type2", "Type3", "Type4", "Type5")

X.matrix <- matrix(NA, nrow=nrow(random.data), ncol=length(include.in.X))
colnames(X.matrix) <- include.in.X
rownames(X.matrix) <- rownames(random.data)

percentage.of.all.cells <- 0.2

# samples that are included in X must not be used in the training set!
samples.to.remove <- c()

for(l.type in include.in.X){
  all.of.type <- names(indicator.list)[which(indicator.list == l.type)]
  chosen.for.X <- sample(x = all.of.type,
                         size = length(all.of.type) * percentage.of.all.cells,
                         replace = FALSE)
  samples.to.remove <- c(samples.to.remove, chosen.for.X)

  average <- rowSums(random.data[, samples.to.remove])
  X.matrix[, l.type] <- average
}

# here, I declare "Type1" as Tumor cells, and all other as immune cells
special.samples <- c("Type1")
all.samples <- unique(indicator.list)
sample.names <- all.samples[- which(all.samples \%in\% special.samples)]

train.mat <- random.data[, -which(colnames(random.data) \%in\% samples.to.remove)]
indicator.list <- indicator.list[-which(!names(indicator.list) \%in\% colnames(train.mat))]


training.data <- mix.samples.Jitter(sample.names = sample.names,
                                     special.samples = special.samples,
                                     nMixtures = 1e3,
                                     datamatrix = train.mat,
                                     indicator = indicator.list,
                                     singleSpecial = F,
                                     add_jitter = T,
                                     chosen.mean = 1,
                                     chosen.sd = 0.05,
                                     min.amount.samples = 1,
                                     verbose = FALSE,
                                     included.in.X = include.in.X)

# wrapper for gradient:
DTD.grad.wrapper <- function(tweak){
   X <- X.matrix
   Y <- training.data$mixtures
   C <- training.data$quantities
   grad <- Trace.H.gradient(X = X, Y = Y, C = C, gamma.vec = tweak)
   return(grad)
}
# wrapper for evaluate corelation:
DTD.evCor.wrapper <- function(tweak){
   X <- X.matrix
   Y <- training.data$mixtures
   C <- training.data$quantities
   loss <- evaluate_cor(X = X, Y = Y, C = C, tweak = tweak)
}

start_tweak <- rep(1, nrow(X.matrix))
start_cor <- 1 - DTD.evCor.wrapper(start_tweak)
cat("Starting correlation: ", start_cor, "\\n")

catch <- descent_generalized_fista(tweak_vec = start_tweak,
                                   F.GRAD.FUN = DTD.grad.wrapper,
                                   ST.FUN = soft_thresholding,
                                   FACTOR.FUN = nesterov_faktor,
                                   EVAL.FUN = DTD.evCor.wrapper,
                                   line_search_speed = 2,
                                   maxit = 1e3,
                                   save_all_tweaks = T)



}
