% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/function_FISTA.R
\name{descent_generalized_fista}
\alias{descent_generalized_fista}
\title{Descent generalized FISTA \cr
(fast iterative shrinkage thresholding algorithm)}
\usage{
descent_generalized_fista(tweak.vec, lambda = 0, maxit = 100,
  learning.rate = NA, F.GRAD.FUN, ST.FUN = soft_thresholding,
  FACTOR.FUN = nesterov_faktor, EVAL.FUN, NORM.FUN = identity,
  line.search.speed = 2, cycles = 5, save.all.tweaks = FALSE,
  use.restart = TRUE, verbose = TRUE,
  NESTEROV.FUN = positive_subspace_pmax, stop.crit.threshold = 1e-05)
}
\arguments{
\item{tweak.vec}{numeric vector, with which the minimization algorithm starts}

\item{lambda}{non-negative float, regularization factor for ST.FUN function, defaults to 0}

\item{maxit}{integer, maximum number of iterations for the iterative
minimization, defaults to 1000}

\item{learning.rate}{float, step size while learning. Defaults to NA. If it is NA, the learning rate will
be estimated as published by Barzilai & Borwein 1988}

\item{F.GRAD.FUN}{function with one parameter: vector with same length as
tweak.vec, and one return argument: vector with same length as tweak.vec
For a given vector, it returns the gradient of the loss-function}

\item{ST.FUN}{function, with one parameter: vector with same length as
tweak.vec, and one return argument: vector with same length as tweak.vec
implementation of the proximal operator for the chosen Loss-function. Here
named soft thresholding function. Defaults to soft_thresholding}

\item{FACTOR.FUN}{function, with a integer as input, and a float as return
value. This function returns the factor the nesterov correction
extrapolates. Defaults to nesterov_faktor}

\item{EVAL.FUN}{function, with one parameter: vector with same length as
tweak.vec, and a single float as return. This function evaluates the loss
function.}

\item{NORM.FUN}{function, with one parameter: After each step the tweak vector will be normed/scaled.
If no scaling should be done, set NORM.FUN to identity. Defaults to identity.}

\item{line.search.speed}{numeric, factor with which the learning rate changes,
if the optimium has not been found. Defaults to 2.}

\item{cycles}{integer, in each iteration one gradient is calculated. To find the
best step size, we do "cycles" steps, and evaluate each of them to find the best step size.
Defaults to 5}

\item{save.all.tweaks}{boolean, should all tweak vectores during all iterations be stored. Defaults to FALSE}

\item{use.restart}{boolean, restart the algorithm if the update was not a descent step. Defaults to TRUE}

\item{verbose}{boolean, should information be printed to console. Defaults to TRUE}

\item{NESTEROV.FUN}{function, applied to the nesterov extrapolation.
In DTD all g must be positive. Therefore we set default to a positive_subspace_pmax wrapper function}

\item{stop.crit.threshold}{numeric value. The change in either the gradient
or nesterov step has to be at least 'stop.crit.threshold'.}
}
\value{
list, including
\itemize{
   \item "Converge", numeric vector, EVAL.FUN in every step
   \item "Tweak" numeric vector, the best distinguishing vector
   \item "Lambda", numeric value, used \eqn{\lambda}
   \item depending on "save.all.tweaks": "History", numeric matrix, "Tweak" vector of every step
}
}
\description{
descent_generalized_fista takes as input a vector, a gradient function and an
evaluation function (and some additional parameters/functions). Then, it
iteratively minimizes the tweak vector via FISTA (Beck and Teboulle 2009). Basically,
the following equations are used:\cr
# prelimary initialization step\cr
for k = 2,..., maxit:\cr
\itemize{
    \item grad = F.GRAD.FUN(y_vec)\cr
    \item# find best step size between 0 and learning.rate\cr
         for step.size = 0,..., learning.rate:\cr
    \itemize{
        \item u = ST.FUN(y_vec - step.size * grad, step.size * lambda)\cr
        \item eval = EVAL.FUN(u)
    }
    \item # only keep u with minimal eval\cr
    \item tweak.old = tweak.vec\cr
    \item # if it was a descent step: \cr tweak.vec = u
    \item # if it was not a descent step: \cr tweak.vec = tweak.vec \cr#(and restart as suggested in  Oâ€™Donoghue & Candes (2012))
    \item # Nesterov extrapolation:
    \item nesterov.direction = tweak.vec - tweak.old
    \item # find best extrapolation size bewteen 0 and FACTOR.FUN(k):\cr
         for ne = 0 ,... FACTOR.FUN(k):\cr
    \itemize{
        \item y_vec = u_vec + ne * nesterov.direction
        \item eval = EVAL.FUN(y_vec)
    }
    \item # only keep y_vec with minimal eval
    \item stop criterion: if the descent in either the gradient
    or the nesterov step was below a critical value, then stop.
}
}
\details{
The gradient and evaluation function both take only one argument, the soft thresholding function two.
If your gradient, evaluation or soft thresholding function require more arguments, please write a wrapper.
Exemplary wrapper functions can be found in the examples.
}
\examples{
library(DTD)
random.data <- generate_random_data(n.types = 10,
                                    n.samples.per.type = 100,
                                    n.features = 200,
                                    sample.type = "Cell",
                                    feature.type = "gene")

# normalize all samples to the same amount of counts:
normalized.data <- normalize_to_count(random.data)

# extract indicator list.
# This list contains the Type of the sample as value, and the sample name as name
indicator.list <- gsub("^Cell[0-9]*\\\\.", "", colnames(random.data))
names(indicator.list) <- colnames(random.data)

# extract reference matrix X
# First, decide which cells should be deconvoluted.
# Notice, in the mixtures there can be more cells than in the reference matrix.
include.in.X <- paste0("Type", 2:5)

percentage.of.all.cells <- 0.2
sample.X <- sample_random_X(included.in.X = include.in.X,
                            pheno = indicator.list,
                            exp.data = normalized.data,
                            percentage.of.all.cells = percentage.of.all.cells)
X.matrix <- sample.X$X.matrix
samples.to.remove <- sample.X$samples.to.remove

# all samples that have been used in the reference matrix, must not be included in
# the test/training set
remaining.mat <- random.data[, -which(colnames(random.data) \%in\% samples.to.remove)]
train.samples <- sample(x = colnames(remaining.mat),
                       size = ceiling(ncol(remaining.mat)/2),
                       replace = FALSE)
test.samples <- colnames(remaining.mat)[which(!colnames(remaining.mat) \%in\% train.samples)]

train.mat <- remaining.mat[, train.samples]
test.mat <- remaining.mat[, test.samples]

indicator.train <- indicator.list[names(indicator.list) \%in\% colnames(train.mat)]
training.data <- mix_samples(exp.data = train.mat,
                             pheno = indicator.train,
                             included.in.X = include.in.X,
                             n.samples = 500,
                             n.per.mixture = 50,
                             verbose = FALSE)

indicator.test <- indicator.list[names(indicator.list) \%in\% colnames(test.mat)]
test.data <-  mix_samples(exp.data = test.mat,
                          pheno = indicator.test,
                          included.in.X = include.in.X,
                          n.samples = 500,
                          n.per.mixture = 50,
                          verbose = FALSE)

GRADIENT.WRAPPER <- function(tweak,
                            X=X.matrix,
                            train.list = training.data){
 grad <- gradient_cor_trace(X = X,
                    Y = train.list$mixtures,
                    C = train.list$quantities,
                    tweak = tweak,
                    estimate.c.type = "direct")
 return(grad)
}
EVAL.WRAPPER <- function(tweak,
                            X=X.matrix,
                            train.list = training.data){

 loss <- evaluate_cor(X.matrix = X,
                      new.data = train.list$mixtures,
                      true.compositions = train.list$quantities,
                      DTD.model = tweak,
                      estimate.c.type = "direct")
 return(loss/ncol(X))
}



start.tweak <- rep(1, nrow(X.matrix))
names(start.tweak) <- rownames(X.matrix)
# Standard model:
cor.train.standard.model <- EVAL.WRAPPER(start.tweak)
cat("Correlation on train data, with standard model: ", cor.train.standard.model, "\\n")

# Train a single deconvolution model:
catch <- descent_generalized_fista(
              tweak.vec = start.tweak,
              F.GRAD.FUN = GRADIENT.WRAPPER,
              EVAL.FUN = EVAL.WRAPPER)

# evaluate model on test data:
cor.test <- evaluate_cor(
    X.matrix = X.matrix,
    new.data = test.data$mixtures,
    true.compositions = test.data$quantities,
    DTD.model = catch,
    estimate.c.type = "direct"
    )/ncol(X.matrix)
cat("Correlation on test data: ", cor.test, "\\n")
}
