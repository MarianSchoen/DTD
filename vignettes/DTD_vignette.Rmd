---
  title: "Loss-function learning for digital tissue deconvolution"
  author: "Marian SchÃ¶n"
  date: "`r Sys.Date()`"
  output: 
    rmarkdown::html_vignette: 
      toc: TRUE
  bibliography: bibliography.bib
  vignette: >
    %\VignetteIndexEntry{Vignette Title}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# What is DTD?
The name of the "DTD" package is an acronym for Digital Tissue Deconvolution. Here, we provide functions and routines with which a DTD model can be trained. 

# How to use? 
In the following examples, I demonstrate how data has to look like, in order to be used for loss-function learning digital tissue deconvolution. After that, I give examples on how to process the data, how to generate a test and training set, how to train the g vector, and finally, how to visualize the result. 
First of all, load the library: 
```{r}
  library(DTD)
```
## Data generation
The DTD package needs expression measurements of sorted cells to train a g-vector. We did not include any data in the DTD package. In order to demonstrate all functions, and to show users how the data needs to be processed, we included a `generate.random.data` function.
Basically, this function has 3 key arguments:

- nTypes: integer, how many different types should be included in the data set. 
- nSamples.perType: integer, ow many samples should be generated per type.
- nFeatures: integer, how many features should be included.

In the following we generate data consisting of 5 different cell types, with 100 features. For each cell type we generate 5 samples. 
```{r}
  number.types <- 5
  number.samples.per.type <- 10
  random.data <- generate.random.data(nTypes = number.types, 
                                      nSamples.perType = number.samples.per.type, 
                                      nFeatures = 100)
  print(random.data[1:5, 1:5])
  
  # for further exemplary visualization 
  example.index <- c(1, 12, 23, 34, 44)
```

The object `random.data` is a numeric matrix with `r nrow(random.data)` rows (features), and `r ncol(random.data)` columns (samples). Notice that the colnames of `random.data` indicate the type of cell:
```{r}
  print(colnames(random.data)[example.index])
```

The expression matrix `random.data` now looks similar to expression measurements published by @Tirosh2016 OR DO I HAVE ANY OTHER EXAMPLE. 

## Data processing
The first step in processing the raw expression matrix is to undo log transformation. This is necessary because our DTD algorithm works on an additive scale, not a multiplicative. 
```{r}
  if(max(random.data) < 25){ 
    random.data <- 2^random.data
  }
```
Next, we scale every sample of the data set to a common number of counts. This brings all samples to the same scale, and makes them comparable. In the DTD package the function `normalizeToCount` can be used: 
```{r}
  # In random.data the number of counts differs over all samples: 
  apply(random.data, 2, sum)[example.index]
  normalized.data <- normalizeToCount(random.data)
  # In normalized.data all samples share the same number of counts: 
  apply(normalized.data, 2, sum)[example.index]
```

In addition to the normalized data matrix the DTD algorithm needs a `indicator.list`:
```{r}
 indicator.list <- gsub("^Cell([0-9])*.", "", colnames(normalized.data))
 names(indicator.list) <- colnames(normalized.data)
 print(indicator.list[example.index])
```
`indicator.list` needs to be a named list. Each entry assigns the cell type (as value of the list) to every sample (names of the list) in the `random.data`. 

## Reference matrix X

 Next, we select the reference matrix X. In our data set there are `r number.types` different cell types. In this example we do not distinguish between all of them. The samples we want to deconvolute are: 
```{r}
  include.in.X <- c("Type2", "Type3", "Type4", "Type5")
```
For each of these types we need a reference profile x, which will be gathered together and become the reference matrix X: 
```{r}
  X.matrix <- matrix(NA, nrow=nrow(normalized.data), ncol=length(include.in.X))
  colnames(X.matrix) <- include.in.X
  rownames(X.matrix) <- rownames(normalized.data)
```
For each cell type its reference profil will be calculated as the average over a fraction of all samples in the whole data set:
```{r}
  percentage.of.all.cells <- 0.2
  samples.to.remove <- c()
  for(l.type in include.in.X){
    # get sample names of all cells of type "l.type" 
    all.of.type <- names(indicator.list)[which(indicator.list == l.type)]
    
    # randomly sample some cells
    chosen.for.X <- sample(x = all.of.type,
                           size = ceiling(length(all.of.type) * percentage.of.all.cells),
                           replace = FALSE)
    
    # Add those cells which will be included in X to the list of samples.to.remove 
    samples.to.remove <- c(samples.to.remove, chosen.for.X)

    # for each gene average over the selected 
    average <- rowSums(normalized.data[, chosen.for.X, drop = F])
    X.matrix[, l.type] <- average
 }
```
Notice that every sample which has been used in the reference matrix X must not be included in the training or test set! In order to keep track of samples which must not be used furthermore we introduced the variable `samples.to.remove`.
```{r}
  train.mat <- random.data[, -which(colnames(random.data) %in% samples.to.remove)]
```


## Mixing training and test set

The loss-function learning DTD algorithm trains a g-vector on a trainig set. Basically, the training set is a list of two matrices. 

- Y, or mixtures matrix. Y has as many rows (=features) as `X.matrix` or `normalized.data`. Every column y of Y is a mixture of the cells in the data set.
- C, or quantities matrix. C has as many rows as there are cells in `X.matrix`. Every column of C holds the distribution of the cells in each mixture of Y. 

In this package we provide two methods to generate the training set. One method randomly samples many cells of the complete data set, and averages over them. This is the prefered method. However, if there are only a few samples per cell type (in average below 10 samples per type) we recommend to mix samples using jitter. For both methods we provide functions within the DTD package.

### Many samples --> directly

The `mix.samples` function needs the following arguments:

- gene.mat: numeric matrix, here `train.mat`
- pheno: named list of strings, here `indicator.list`
- included.in.X: list of strings, here include.in.X
- nSamples: integer, how many in-silicio mixtures should be generated. 
- nPerMixture: integer, how many cells should be included in each mixture
- verbose: boolean, should output be printed. 

```{r}
  training.data <- mix.samples(gene.mat = train.mat,
                               pheno = indicator.list,
                               included.in.X = include.in.X, 
                               nSamples = 1e3, 
                               nPerMixture = 5, 
                               verbose = F)
  str(training.data)
```
Remark that all types are included in the mixtures, but only the distributions of the cell within the X matrix are reported in the quantities matrix. 

### Less samples --> with jitter
The `mix.samples.jitter` function needs a bit more preliminary work. 
It takes the following arguments: 

- special.names: list of strings. Cell types included in the special.names list will occur with higher quantities in the mixtures 
- sample.names: list of strings. Cell types included in the sample.names list will occur with lower quantities in the mixtures

The idea behind splitting all samples in the data set into `special.names` and `sample.names` is: 
In a tumor tissue malignant cells make up ~75% of all cells. In contrast to that, certain immune cells (e.g. a macrophage subtype) make up only ~5 % of all cells. 
In the training set imbalanced quantities should be reflected. 

- nSamples: integer, how many in-silicio mixtures should be generated. 
- datamatrix: numeric matrix, here `train.mat`
- pheno: named list of strings, here `indicator.list`
- verbose: boolean,  should output be printed. 
- singleSpecial: boolean, if `FALSE`  all `special.samples` will be included in all mixtures, or if `TRUE`  only one `special.sample` will be included per mixture. This option should be used if `special.samples` represent different tumor types. 
- add_jitter: boolean, should the mixtures be multiplied with jitter. 
- chosen.mean: numeric, mean of jitter
- chosen.sd: numeric, standard deviation of jitter
- min.amount.samples: integer, how many samples have to be present such that it averages over them, instad of taking only 1. 
- included.in.X: list of strings, here include.in.X

```{r}
  # Here, we set "Type1" to be special:
  special.samples <- c("Type1")
  # and all other to be normal: 
  all.samples <- unique(indicator.list)
  sample.names <- all.samples[- which(all.samples %in% special.samples)]
  
  # reduce indicator list to those samples included in training: 
  indicator.list <- indicator.list[names(indicator.list) %in% colnames(train.mat)]
  
  training.data.jitter <- mix.samples.jitter(sample.names = sample.names,
                                             special.samples = special.samples, 
                                             nSamples = 1e3, 
                                             datamatrix = train.mat, 
                                             pheno = indicator.list, 
                                             verbose = F, 
                                             add_jitter = T, 
                                             included.in.X = include.in.X)
```

## Train g vector

Now we generated a training set, and are nearly set to start the optimizing procedure. 
~~~In this package we implemented a adjusted FISTA algorithm to minimize our Loss-function. FISTA (fast iterative shrinkage thresholding algorithm) is a iterative proximal gradient method with Nesterov acceleration. Basically, the FISTA algorithm can be described as a extrapolation/correction/nesterov step after a proximal gradient descen step. In the loss-function learning DTD context the computational expensive part is the calculation of the gradient.~~~ 


We provide a function with the following arguments: 

- tweak_vec: numeric vector, with which the minimization starts. 
- maxit: integer, maximum number of iterations. 
- learning.rate: float, initial step size while learning. During the algorithm it may in or decreased. 
- F.GRAD.FUN: function with one parameter: a vector with same length as tweak_vec, and one return value with the same length. For a given vector, it returns the gradient of the loss function. 
- EVAL.FUN: function with one parameter: a vector with same length as tweak_vec, and a float as return value. For a given vector, it returns the value of the loss function. 

Both functions take only one list as input argument, which is the `tweak_vec`. If the gradient function takes more than one argument, a wrapper function is needed: 
```{r}
  DTD.grad.wrapper <- function(tweak){
      X <- X.matrix
      Y <- training.data$mixtures
      C <- training.data$quantities
      grad <- Trace.H.gradient(X = X, Y = Y, C = C, gamma.vec = tweak)
      return(grad)
  }
  DTD.evCor.wrapper <- function(tweak){
      X <- X.matrix
      Y <- training.data$mixtures
      C <- training.data$quantities
      loss <- evaluate_cor(X = X, Y = Y, C = C, tweak = tweak)
  }
```

Now, the optimization procedure can be started: 

```{r}
  start_tweak <- rep(1, nrow(X.matrix))
  catch <- descent_generalized_fista(tweak_vec = start_tweak,
                                      F.GRAD.FUN = DTD.grad.wrapper,
                                      ST.FUN = soft_thresholding,
                                      FACTOR.FUN = nesterov_faktor,
                                      EVAL.FUN = DTD.evCor.wrapper,
                                      line_search_speed = 2,
                                      maxit = 1e2,
                                      save_all_tweaks = T, 
                                      verbose = F)
```



## Visualize results

# References
